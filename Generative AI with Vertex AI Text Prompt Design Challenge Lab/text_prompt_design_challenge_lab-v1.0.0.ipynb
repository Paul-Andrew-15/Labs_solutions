{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Text Prompt Design: Challenge Lab\n",
    "\n",
    "## Overview\n",
    "\n",
    "This challenge lab is designed to test your knowledge of calling Gemini and utilizing a few fundamental text prompt design techniques.\n",
    "\n",
    "Two featured guides on prompting from the Google Cloud documentation are:\n",
    "\n",
    "1. [Overview of prompting strategies](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies) from the Generative AI on Vertex AI documentation.\n",
    "\n",
    "2. [Prompt design strategies](https://ai.google.dev/gemini-api/docs/prompting-strategies) from the Gemini API documentation.\n",
    "\n",
    "Both contain good tips. You are encouraged to **bookmark them**.\n",
    "\n",
    "## Objective\n",
    "You will demonstrate your ability to:\n",
    "\n",
    "- Initialize Vertex AI in your environment\n",
    "- Load a generative model\n",
    "- Guide model output with a persona\n",
    "- Extract information to a schema\n",
    "- Stay on topic with fallback responses\n",
    "- Use examples to influence the model's response\n",
    "\n",
    "Some of the following Python notebook cells have missing or incomplete code sections and tasks that need to be completed, indicated by the code comments starting with `# TODO`. Your challenge is to complete each cell, run it to test for correctness, and then move on. When all the cells are working, you have completed the challenge.\n",
    "\n",
    "**Note:** If you need help, [this notebook demonstrates getting started using Gemini in Python](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_python.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "## Task 1. Install, import & initialize the Vertex AI SDK and a generative model\n",
    "\n",
    "1. Install the Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the following pip command\n",
    "%pip install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "2. Restart your notebook kernel.\n",
    "3. Import the following:\n",
    "- the Vertex AI SDK\n",
    "- the class to instantiate a generative model from the Vertex AI generative models module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpdnPWaTK27C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"qwiklabs-gcp-04-a0cddc85f1d0\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "4. Initialize Vertex AI with your project ID and a location (you can use like `us-central1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Insert the required steps here\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Image,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8DUcgKJK27C"
   },
   "source": [
    "5. Instantiate a generative model and save it to the `generative_model` variable. For this notebook, use `gemini-pro` as your model version. When instantiating the model, pass a `generation_config` parameter with the temperature set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Insantiate a \"gemini-pro\" model with a configured temperature of 0.\n",
    "#generative_model =\n",
    "model = GenerativeModel(\"gemini-1.5-pro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3YN2rf1K27C"
   },
   "source": [
    "6. Complete the TODO's in this function, which you will use for the rest of the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joXl2V2JK27D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def print_response(prompt):\n",
    "\n",
    "#     # TODO: Complete this line to generate a response to the prompt:\n",
    "#     response = generative_model.\n",
    "\n",
    "#     # TODO: Complete this line to print only the text of the model's response,\n",
    "#     # not the additional response metadata.\n",
    "    \n",
    "# response = model.generate_content(\"Why is the sky blue?\")\n",
    "\n",
    "# print(response.text)\n",
    "#     print()\n",
    "\n",
    "def print_response(prompt):\n",
    "    # Generate a response to the prompt using the generative model\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    # Print only the text of the model's response\n",
    "    print(response.text)\n",
    "    print()\n",
    "\n",
    "# Example usage\n",
    "print_response(\"Why is the sky blue?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEAJ0ipmbndQ"
   },
   "source": [
    "## Task 2. Personas\n",
    "\n",
    "1. Run the following cell to see the default response to this prompt.\n",
    "\n",
    "2. Then tweak the prompt by asking the model to take on the persona of an **energetic, inspiring personal trainer** who can get users **excited to work out their leg muscles**. Note the difference in vocabulary and tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEo0QhauK27D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Improve the personality of the response by assigning the suggested persona.\n",
    "# prompt = \"What are some good leg exercises?\"\n",
    "\n",
    "# print_response(prompt)\n",
    "\n",
    "\n",
    "# Original prompt to see the default response\n",
    "prompt = \"What are some good leg exercises?\"\n",
    "print_response(prompt)\n",
    "\n",
    "# Tweaked prompt for the energetic, inspiring personal trainer persona\n",
    "tweaked_prompt = \"As an energetic and inspiring personal trainer, can you get me excited about some amazing leg exercises that will really pump up my muscles?\"\n",
    "\n",
    "print_response(tweaked_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueKZMheeK27D"
   },
   "source": [
    "## Task 3. Be specific + constrain the output format\n",
    "\n",
    "1. Have the model convert the following text of cooking ingredients to a YAML format. Each ingredient should be listed as a dictionary with keys for **ingredient** and **quantity** populated with the correct value given the ingredients in the following recipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyJASDrsK27D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instructions for the model to convert the text into YAML format\n",
    "instructions = (\n",
    "    \"Please convert the following list of cooking ingredients into YAML format. \"\n",
    "    \"Each ingredient should be represented as a dictionary with the keys 'ingredient' and 'quantity'. \"\n",
    "    \"Ensure that the output is properly formatted as valid YAML.\"\n",
    ")\n",
    "\n",
    "ingredients = \"\"\"\n",
    "    Ingredients:\n",
    "    * 9 egg whites\n",
    "    * 3/8 tsp Cream of Tartar\n",
    "    * 1 1/2 tbs Vinegar\n",
    "    * 1 1/2 tsp Vanilla\n",
    "    * 3 cups Sugar\n",
    "    * 1 quart Heavy whipping cream\n",
    "    * 3 boxes Strawberries\n",
    "    \"\"\"\n",
    "\n",
    "# Combine instructions and ingredients into a single prompt\n",
    "prompt = instructions + \"\\n\\n\" + ingredients\n",
    "\n",
    "# Print the response based on the prompt\n",
    "print_response(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xRk8HTLK27D"
   },
   "source": [
    "## Task 4. Use a fallback response\n",
    "\n",
    "1. Adjust the prompt below to specify that the model should only answer questions related to historical landmarks. If a user askes about something else, the model should respond with the message: `Sorry, I only answer questions about historical landmarks!`\n",
    "\n",
    "2. Adjust your instructions until the model declines to answer the `user_query` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6InaQ3OpK27D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instructions to limit the response to historical landmarks and provide a fallback for off-topic queries\n",
    "instructions = (\n",
    "    \"You are a history tour guide. You only answer questions related to historical landmarks. \"\n",
    "    \"If the question is about something else, respond with: 'Sorry, I only answer questions about historical landmarks!' \"\n",
    "    \"Answer the user's question: {user_query}\"\n",
    ")\n",
    "\n",
    "user_query = \"How can I attract butterflies to my garden?\"\n",
    "\n",
    "# Print the response based on the adjusted prompt\n",
    "print_response(instructions.format(user_query=user_query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFolKJjNK27D"
   },
   "source": [
    "## Task 5. Make results more specific with examples\n",
    "\n",
    "1. Run the code cell below to see the model's response as-is.\n",
    "\n",
    "2. Imagining you work for a bicycle tour company, modify each of the example outputs below to include a bicycle.\n",
    "\n",
    "3. Re-run the code cell to make sure the model generates a bicycle-themed response. Leave the instructions alone and tweak your examples until you get such a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsjp39ZWK27D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Modify the examples below to guide the model to always generate\n",
    "# recommendations involving bicycles.\n",
    "\n",
    "prompt = model.generate_content(f\"\"\"\n",
    "    <INSTRUCTIONS>\n",
    "    Give a tourist recommendation for the input city.\n",
    "    </INSTRUCTIONS>\n",
    "\n",
    "    <EXAMPLES>\n",
    "    Input: Paris\n",
    "    Output: Take a bicycle to the Louvre and then to Montmartre.\n",
    "\n",
    "    Input: Washington D.C.\n",
    "    Output: Drive your rental bicycle to the Lincoln Memorial.\n",
    "\n",
    "    Input: New York City\n",
    "    Output: Walk along the bicycle.\n",
    "    </EXAMPLES>\n",
    "\n",
    "    <INPUT CITY>\n",
    "    Bangalore\n",
    "    </INPUT CITY>\"\"\")\n",
    "\n",
    "print_response(prompt.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvbIXnwqK27D"
   },
   "source": [
    "## Congratulations!\n",
    "\n",
    "If you have completed the steps above, you have demonstrated your ability to use several prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_prompt_design_challenge_lab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
